from __future__ import division
import torch
import torch.nn as nn
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torch.autograd import Variable
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import time
import cv2
import os

from save_model import sour_cls, test_loader, source_m, classifier
model=sour_cls()
model=torch.load('/home/hankeji/Desktop/new_model/ori_sourec_m.pkl')
model=model.classifier.cpu()
optimizer=optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)


def atest(epoch):
    n=0
    #data_adv=torch.FloatTensor()
    #target_adv=torch.FloatTensor()
    for batch_idx, (data, target) in enumerate(test_loader):
        print ('This is {}_epoch: {}_th batch'.format(epoch, batch_idx))
        data, target= Variable(data, requires_grad=True), Variable(target)
        data = Variable(data.data, requires_grad=True)
        '''
        if batch_idx%10==0:#show the img generated by source_m
            tmp_img=model.source_m(data)
            tmp_img=tmp_img.cpu().data.numpy()[0]
            tmp_img=np.reshape(tmp_img, (28,28))
            cv2.imshow('kk', tmp_img)
            cv2.waitKey(200)
            cv2.destroyAllWindows()
        '''

        output=model(data)
        loss=F.nll_loss(output, target)
        pred = output.data.max(1)[1]  # get the index of the max log-probability
        n += pred.eq(target.data).cpu().sum()
        loss.backward()
        tmp_adv=data+torch.mul(torch.sign(data.grad), 0.2)
        if batch_idx==0:
            data_adv=tmp_adv
            target_adv=target
        else:
            data_adv=torch.cat((data_adv, tmp_adv),0)
            target_adv=torch.cat((target_adv, target), 0)


    print data_adv.size()
    print target_adv.size()
    torch.save(data_adv, '/home/hankeji/Desktop/ADDA/data/FGSM_0.2.pkl')
    torch.save(target_adv, '/home/hankeji/Desktop/ADDA/data/label.pkl')
    return data_adv, target_adv

if __name__=='__main__':
    atest(0)
